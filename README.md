# spark_homework
云计算实践小作业
## first.py ##
从给定的文件中（evaluate.txt）读取所有数值，返回从大到小排列的Top K个数值。给定文件的第一行是K值，后面每一行是一个数值。
## second.py ##
1. 从指定的文本文件中读取K值、要处理的文件地址和结果写入的文件路径；该文本文件第一行为K值，第二行为数据文件的地址，第三行为结果要写入的文件路径，设为P；在程序中输出结果的时候将结果写入P路径下的对应学号的txt文件，即P+<学号>.txt。

2. 读取集群上文本文件，对文本文件中内容进行Top K处理。文本文件中包含多行的<id, value>对，每一行就是一个<id, value>对，同一个id可能会出现多次，求value Top K的时候要对同一个id对应的value进行求和。最终要输出value Top K的id列表（id包括英文字母'id'），按照顺序输出（一行一个id，不用逗号，直接换行）。

## third ##
从指定的MongoDB数据库中读取数据，模拟流数据，使用Spark Streaming方法统计数据流中某个类型词语出现的频次。

#### third1.py ####
模拟流数据：从该collection中的第一个数据开始，每次读取50个数据条目的head->text内容，每一个text内容为一行，分行写入一个<head+时间戳>.log文件，先放在流数据文件夹下的/tmp文件夹里，写完后移动到流数据文件夹，然后线程sleep 1秒。

#### third2.py ####
1. 自定义一个中国省份、直辖市、自治区的完整列表，作为统计的参考；如果一条数据中没有对应的省份、自治区或直辖市则放过。

2. 流数据处理：使用Spark Streaming监听流数据文件夹，每隔10秒处理一次数据，每一次处理先统计每一个省份在本次处理中出现的次数，将结果作为一行写入结果文件，格式为<省份_次数;省份_次数;...>（尖括号不需要），所有省份在一行；然后再计算出到目前为止，各个省份累计出现的次数，作为另一行写入结果文件，格式也是<省份_次数;省份_次数;...>。所以每一次计算要在结果文件中新写入两行。
